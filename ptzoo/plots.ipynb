{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maenv.ma_scopa_env import MaScopaEnv\n",
    "from pettingzoo.utils import BaseWrapper\n",
    "from pettingzoo.utils.conversions import aec_to_parallel\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "from tlogger import TLogger\n",
    "import random\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    " \n",
    "SIDE = 1\n",
    "\n",
    "class SB3ActionMaskWrapper(BaseWrapper):\n",
    "    \"\"\"Wrapper to allow PettingZoo environments to be used with SB3 illegal action masking.\"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Gymnasium-like reset function which assigns obs/action spaces to be the same for each agent.\n",
    "\n",
    "        This is required as SB3 is designed for single-agent RL and doesn't expect obs/action spaces to be functions\n",
    "        \"\"\"\n",
    "        super().reset()\n",
    "\n",
    "        # Strip the action mask out from the observation space\n",
    "        self.observation_space = super().observation_space(self.possible_agents[0])\n",
    "        self.action_space = super().action_space(self.possible_agents[0])\n",
    "\n",
    "        # Return initial observation, info (PettingZoo AEC envs do not by default)\n",
    "        return self.observe(self.agent_selection), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Gymnasium-like step function, returning observation, reward, termination, truncation, info.\"\"\"\n",
    "        super().step(action)\n",
    "        return super().last()\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"Return only raw observation, removing action mask.\"\"\"\n",
    "        return super().observe(agent)\n",
    "\n",
    "    def action_masks(self):\n",
    "        \"\"\"Separate function used in order to access the action mask.\"\"\"\n",
    "        return self.get_action_mask()\n",
    "\n",
    "def sanity_check(mask):\n",
    "    # Checks that the mask is not malformed. Functions only for a novel enviroment (all with starting cards)\n",
    "    for m in mask:\n",
    "        assert sum(m) == 10\n",
    "\n",
    "    for m in range(len(mask[0])):\n",
    "        mask_sum = np.sum([mask[i][m] for i in range(len(mask))])\n",
    "        assert mask_sum == 1 \n",
    "\n",
    "def mask_fn(env):\n",
    "    return env.get_action_mask()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = []\n",
    "scores_h = []\n",
    "\n",
    "def eval_action_mask(env_fn, num_games=10000, render_mode=None, side= SIDE, tlogger = None):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn\n",
    "    \n",
    "    if side == 0:\n",
    "        sidet = ['player_0', 'player_2']\n",
    "        nsidet = ['player_1', 'player_3']\n",
    "    else:\n",
    "        sidet = ['player_1', 'player_3']\n",
    "        nsidet = ['player_0', 'player_2']\n",
    "\n",
    "    print(\n",
    "        f\"Starting evaluation vs a random agent.\\n\\t!Old RW! agent will play as side: {side} with players: {sidet}\\n\\t!New RW! agent will be players: {nsidet}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        policies = glob.glob(f\"C:/Users/aless/Repos/Rug/P2/ScopAI/ScopaAI_ToM/{env.metadata['name']}*.zip\")\n",
    "        latest_policy = max(\n",
    "            policies, key=os.path.getctime\n",
    "        )\n",
    "\n",
    "        ##LOAD DIFFERENT POLICIES\n",
    "        tomZero = policies[2]\n",
    "        latest_policy = policies[0]\n",
    "        latest_policy = tomZero\n",
    "\n",
    "        print(f\"Loading policy: {latest_policy} amd {tomZero}\")\n",
    "        #print(f\"Loading policy: random amd {latest_policy}\")\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "\n",
    "    model = MaskablePPO.load(tomZero)\n",
    "    model_TOM = MaskablePPO.load(latest_policy)\n",
    "\n",
    "\n",
    "    scores = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    round_rewards = []\n",
    "\n",
    "    for i in tqdm(range(num_games), desc=\"Playing games\"):\n",
    "        env.reset(seed=i)\n",
    "        env.action_space(env.possible_agents[0]).seed(i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "\n",
    "            observation, action_mask = obs, info['action_mask']\n",
    "\n",
    "            if termination or truncation:\n",
    "                # If there is a winner, keep track, otherwise don't change the scores (tie)\n",
    "                if (\n",
    "                    env.rewards[env.possible_agents[0]]\n",
    "                    != env.rewards[env.possible_agents[1]]\n",
    "                ):\n",
    "                    winner = max(env.rewards, key=env.rewards.get)\n",
    "                    if winner == 'player_0' or winner == 'player_2':\n",
    "                        scores['player_2'] += env.rewards[winner] + env.rewards['player_0']\n",
    "                        scores['player_0'] += env.rewards[winner] + env.rewards['player_2']\n",
    "                    elif winner == 'player_1' or winner == 'player_3':\n",
    "                        scores['player_3'] += env.rewards[winner] + env.rewards['player_1']\n",
    "                        scores['player_1'] += env.rewards[winner] + env.rewards['player_3']\n",
    "                    scores_h.append(scores)\n",
    "\n",
    "                      # only tracks the largest reward (winner of game)\n",
    "                # Also track negative and positive rewards (penalizes illegal moves)\n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                # List of rewards by round, for reference\n",
    "                round_rewards.append(env.rewards)\n",
    "                break\n",
    "            else:\n",
    "                us = None\n",
    "                if agent not in sidet:\n",
    "                    # act0 = env.action_space(agent).sample(action_mask.astype(np.int8))\n",
    "                    # act1 = env.action_space(agent).sample(action_mask.astype(np.int8))\n",
    "                    act = int(model.predict(\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "                    #act = random.choice([act0,act1])\n",
    "                    us = tomZero\n",
    "                else:\n",
    "                    # Note: PettingZoo expects integer actions # TODO: readapt!!!! and check the results of what is going on\n",
    "                    act = int(model.predict(\n",
    "                            #observation[:3], action_masks=action_mask <-- this is for not TOM models (observation space size is 3x40)\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "                    us = tomZero\n",
    "\n",
    "                actions.append({'player': agent, 'observation': observation, 'action': act, 'model': us})\n",
    "\n",
    "            \n",
    "            env.step(act)\n",
    "            tlogger.add_tick()\n",
    "    scoresp = env.roundScores()\n",
    "    env.close()\n",
    "\n",
    "    \n",
    "    scorespD = pd.DataFrame(scoresp, columns=[latest_policy.split('\\\\')[1],tomZero.split('\\\\')[1]])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Avoid dividing by zero\n",
    "    if sum(scores.values()) == 0:\n",
    "        winrate = 0\n",
    "    else:\n",
    "        winrate = scores[env.possible_agents[0]] / sum(scores.values())\n",
    "    print(\"Total rewards (incl. negative rewards): \", total_rewards)\n",
    "    print(\"Winrate: \", winrate)\n",
    "    print(\"Final scores: \", scores)\n",
    "    return total_rewards, winrate, scores, scorespD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = f\"F-testing_ToM_s{SIDE}_30k_mappo_scopa_{time.strftime('%m%d-%H%M%S')}\"\n",
    "\n",
    "#experiment_name = f\"Training_ToM_2M_SharedCapturesWdiffRew_mappo_scopa_{time.strftime('%m%d-%H%M%S')}\"\n",
    "\n",
    "tlogger = TLogger(f\"runs/{experiment_name}\")\n",
    "\n",
    "env = MaScopaEnv(tlogger=tlogger, render_mode='human')\n",
    "#env = aec_to_parallel(env)\n",
    "env.reset()\n",
    "\n",
    "#train_action_mask(env_fn=env, writer_log=tlogger.get_log_dir(), steps=2_000_000, seed=41)\n",
    "\n",
    "_, _, scores, scoressp = eval_action_mask(env, num_games=30_000, tlogger=tlogger)\n",
    "\n",
    "plt.bar([f'player_{i}' for i in range(4)], tlogger.scopas_log)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actie = pd.DataFrame(actions)\n",
    "\n",
    "remaining = []\n",
    "mod = []\n",
    "\n",
    "\n",
    "for row in actie['observation']:\n",
    "    remaining.append(sum(row[0]))\n",
    "\n",
    "modelsC = []\n",
    "\n",
    "for row in actie['model']:\n",
    "    if row == 'random':\n",
    "        row = '\\\\random'\n",
    "    modelsC.append(row)\n",
    "\n",
    "actie['turn_remaining'] = remaining\n",
    "actie['model'] = modelsC\n",
    "\n",
    "actie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "# Preload card images for tensorboard:)r\n",
    "def preload_card_images(image_folder = 'C:/Users/aless/Repos/Rug/P2/ScopAI/ScopaAI_ToM/res/cards', scale_factor=0.1):\n",
    "    \"\"\"\n",
    "    Preload all card images and scale them down dramatically to save resources.\n",
    "    Args:\n",
    "        image_folder (str): Path to folder containing card images.\n",
    "        scale_factor (float): Factor by which to scale down images.\n",
    "    Returns:\n",
    "        dict: Dictionary mapping card indices to scaled-down images.\n",
    "    \"\"\"\n",
    "    card_images = {}\n",
    "    for filename in os.listdir(image_folder):\n",
    "        if filename.endswith(\".png\"):\n",
    "            # Extract rank and suit from filename\n",
    "            card_name = filename.split(\".\")[0]  # Remove extension\n",
    "            rank, suit = card_name.split(\"_of_\")\n",
    "            \n",
    "            # Map suit to its corresponding value\n",
    "            suit_values = {\"diamonds\": 30, \"clubs\": 20, \"spades\": 10, \"hearts\": 0}\n",
    "            suit_value = suit_values[suit]\n",
    "\n",
    "            if rank == \"jack\":\n",
    "                rank = 8\n",
    "            elif rank == \"queen\":\n",
    "                rank = 9\n",
    "            elif rank == \"king\":\n",
    "                rank = 10\n",
    "            elif rank == \"ace\":\n",
    "                rank = 1\n",
    "            \n",
    "            # Calculate card index based on the new rule\n",
    "            card_index = int(rank) + suit_value - 1\n",
    "            \n",
    "            # Load and scale down image\n",
    "            image = Image.open(os.path.join(image_folder, filename))\n",
    "            new_size = (int(image.width * scale_factor), int(image.height * scale_factor))\n",
    "            scaled_image = image.resize(new_size)\n",
    "            \n",
    "            # Add to dictionary\n",
    "            card_images[card_index] = scaled_image\n",
    "    return card_images\n",
    "\n",
    "card_images = preload_card_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.offsetbox as offsetbox\n",
    "\n",
    "# Assume 'actie' and 'card_images' are already defined.\n",
    "df = pd.DataFrame(actie)\n",
    "models = df['model'].unique()\n",
    "n_models = len(models)\n",
    "\n",
    "# Create a figure with n_models subplots arranged side by side.\n",
    "fig, axes = plt.subplots(n_models, 1, figsize=(12 * n_models, 8), gridspec_kw={'hspace': 0.3})\n",
    "\n",
    "# If there's only one model, wrap the single axis into a list for consistency.\n",
    "if n_models == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Loop over models and plot each heatmap in its corresponding subplot.\n",
    "for ax, model in zip(axes, models):\n",
    "    # Filter data for the current model.\n",
    "    df_model = df[df['model'] == model]\n",
    "\n",
    "    # Create a pivot table: rows = turn_remaining, columns = action (card index),\n",
    "    # values = counts of occurrences.\n",
    "    action_counts = df_model.pivot_table(\n",
    "        index='turn_remaining',\n",
    "        columns='action',\n",
    "        aggfunc='size',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Sort the index so that the highest turn (e.g., 10) is at the top.\n",
    "    action_counts = action_counts.sort_index(ascending=False)\n",
    "    \n",
    "    \n",
    "    # Plot the heatmap on the current axis.\n",
    "    sns.heatmap(action_counts, annot=True, fmt='d', cmap='magma', ax=ax)\n",
    "    \n",
    "    # Remove the default x-axis tick labels so we can add images instead.\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xlabel('')\n",
    "    \n",
    "    # Add Card Images as Custom X-Axis Labels.\n",
    "    # Use a mixed coordinate system: x in data coordinates, y in axis fraction.\n",
    "    for i, card_index in enumerate(action_counts.columns):\n",
    "        if card_index in card_images:\n",
    "            # Convert the PIL image to a NumPy array.\n",
    "            card_img = np.array(card_images[card_index])\n",
    "            # Create an OffsetImage instance.\n",
    "            im = offsetbox.OffsetImage(card_img, zoom=0.5)\n",
    "            # Position the image centered on the x-cell (i+0.5) and at y = -0.05 in axes fraction.\n",
    "            ab = offsetbox.AnnotationBbox(\n",
    "                im,\n",
    "                (i + 0.5, -0.1),\n",
    "                xycoords=(\"data\", \"axes fraction\"),\n",
    "                frameon=False,\n",
    "                clip_on=True\n",
    "            )\n",
    "            ax.add_artist(ab)\n",
    "        else:\n",
    "            print(f\"Card index {card_index} not found in card_images dictionary.\")\n",
    "    \n",
    "    # Add a title for clarity.\n",
    "    # (Note: The splitting of the model string is kept as in your original code.)\n",
    "    ax.set_title(f\"Action Frequencies by Turn Remaining for Model: {model.split('\\\\')[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig('tom0vsrandom_heatmapfR.svg', bbox_inches= 'tight')\n",
    "plt.savefig('tom0vsrandom_heatmapR.png', bbox_inches= 'tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cumulative score (side0 wins minus side1 wins)\n",
    "cumulative_score = []\n",
    "name_model = scoressp.columns[0]\n",
    "score = 0\n",
    "for res in scoressp[name_model]:\n",
    "    if res == -1:       # Side 0 wins\n",
    "        score += 1\n",
    "    elif res == 1:     # Side 1 wins\n",
    "        score -= 1\n",
    "    # (0, 0) is a tie; score remains unchanged\n",
    "    cumulative_score.append(score)\n",
    "\n",
    "com = pd.DataFrame(cumulative_score, columns=[f'{scoressp.columns[1]}VS{name_model}'])\n",
    "\n",
    "# Save the cumulative score dataframe to a CSV file\n",
    "com.to_csv('tom0vstom1.csv', index=False)\n",
    "\n",
    "# Reimport the CSV file into a new dataframe\n",
    "com_reimported = pd.read_csv('t0vst1/cumulative_score_1.csv')\n",
    "\n",
    "# Plot the cumulative score over episodes\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(com_reimported, linestyle='-', color='blue')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Score')\n",
    "plt.title('ToM 1 VS ToM 0')\n",
    "plt.legend([scoressp.columns[0]])\n",
    "plt.grid(True, axis='y')\n",
    "plt.savefig('tom1vstom0.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
